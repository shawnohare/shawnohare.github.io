<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>statistics on Shawn M. O&#39;Hare</title>
    <link>https://www.shawnohare.com/tags/statistics/</link>
    <description>Recent content in statistics on Shawn M. O&#39;Hare</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>© Shawn M. O’Hare.  &lt;a rel=&#34;license&#34; href=&#34;http://creativecommons.org/licenses/by-sa/4.0/&#34;&gt;Some rights reserved&lt;/a&gt;.</copyright>
    <lastBuildDate>Tue, 15 Oct 2013 15:54:00 +0000</lastBuildDate>
    
	<atom:link href="https://www.shawnohare.com/tags/statistics/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Sample Covariance Matrix</title>
      <link>https://www.shawnohare.com/post/2013/10/6e1ecf39-cf3c-459f-b4bd-74551fcba229/</link>
      <pubDate>Tue, 15 Oct 2013 15:54:00 +0000</pubDate>
      
      <guid>https://www.shawnohare.com/post/2013/10/6e1ecf39-cf3c-459f-b4bd-74551fcba229/</guid>
      <description>Suppose \(X_1, \dots, X_p\) are random variables and \(X:=(X_1, \dots, X_p)\) is the random vector of said random variables. Given a sample \(\mathbf x_1, \dots \mathbf x_N\) of \(X\), how do we obtain an estimate for the covariance matrix \(\text{Cov}(X)\)? To this end, let \(\mathbf X=( \mathbf X_{ij})\) denote the \(N \times p\) data matrix whose \(k\)th row is \(\mathbf x_k\) and let \(\bar x_i\) denote the sample mean of \(X_i\).</description>
    </item>
    
    <item>
      <title>Linear Models</title>
      <link>https://www.shawnohare.com/post/2013/10/f2a88cf3-334d-4ad2-b572-9be51bfd57f9/</link>
      <pubDate>Fri, 11 Oct 2013 17:59:00 +0000</pubDate>
      
      <guid>https://www.shawnohare.com/post/2013/10/f2a88cf3-334d-4ad2-b572-9be51bfd57f9/</guid>
      <description>&lt;p&gt;Suppose we think that a random variable &lt;span  class=&#34;math&#34;&gt;\(Y\)&lt;/span&gt; (the response) is linearly dependent on random variables &lt;span  class=&#34;math&#34;&gt;\(X_1, \dots, X_p\)&lt;/span&gt;, where &lt;span  class=&#34;math&#34;&gt;\(p\)&lt;/span&gt; is some integer.    We can model this by assuming that&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\begin{equation*}
  Y =\underbrace{ \beta_0 + \sum_{j=1}^p \beta_j X_j}_{\widehat Y} + \epsilon,  
\end{equation*}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;where &lt;span  class=&#34;math&#34;&gt;\(\beta = (\beta_0, \dots, \beta_p)\)&lt;/span&gt; is the parameter vector for said model and &lt;span  class=&#34;math&#34;&gt;\(\epsilon\)&lt;/span&gt; is a distribution for the residual error between the true value of &lt;span  class=&#34;math&#34;&gt;\(Y\)&lt;/span&gt; and the linear prediction &lt;span  class=&#34;math&#34;&gt;\(\widehat Y\)&lt;/span&gt;.  We assume that &lt;span  class=&#34;math&#34;&gt;\(\epsilon\)&lt;/span&gt; has mean &lt;span  class=&#34;math&#34;&gt;\(0\)&lt;/span&gt;. Letting &lt;span  class=&#34;math&#34;&gt;\(\mathbf x\)&lt;/span&gt; denote the random vector &lt;span  class=&#34;math&#34;&gt;\((1,X_1, \dots, X_p)\)&lt;/span&gt;, this model can be rewritten&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\begin{equation*}
  Y = \mathbf x \cdot \beta + \epsilon,   \quad \text{or} \quad   y(\mathbf x ) = \mathbf x \cdot \beta + \epsilon, 
\end{equation*}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;where &lt;span  class=&#34;math&#34;&gt;\(- \cdot -\)&lt;/span&gt; represents the standard inner product on &lt;span  class=&#34;math&#34;&gt;\(\mathbb R^{p+1}\)&lt;/span&gt;.    One often assumes, as we do now, that &lt;span  class=&#34;math&#34;&gt;\(\epsilon\)&lt;/span&gt; has a Gaussian distribution.  The probability density function &lt;span  class=&#34;math&#34;&gt;\(p\)&lt;/span&gt; for &lt;span  class=&#34;math&#34;&gt;\(Y\)&lt;/span&gt; then becomes&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\begin{equation*}
  p(y \mid \mathbf x, \beta) = \mathcal N(y \mid \mu (\mathbf x), \sigma^2 (\mathbf x)) 
\end{equation*}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;where &lt;span  class=&#34;math&#34;&gt;\(\mu(\mathbf x)  = \mathbf x \cdot \beta\)&lt;/span&gt;.  We will assume that the variance &lt;span  class=&#34;math&#34;&gt;\(\sigma^2(\mathbf x)\)&lt;/span&gt; is a constant &lt;span  class=&#34;math&#34;&gt;\(\sigma^2\)&lt;/span&gt;.   Suppose we make the &lt;span  class=&#34;math&#34;&gt;\(N\)&lt;/span&gt; observations that &lt;span  class=&#34;math&#34;&gt;\(Y = y^{(i)}\)&lt;/span&gt; when &lt;span  class=&#34;math&#34;&gt;\(X_j = x^{(i)}_j\)&lt;/span&gt; for &lt;span  class=&#34;math&#34;&gt;\(j=1, \dots, p\)&lt;/span&gt; and &lt;span  class=&#34;math&#34;&gt;\(i = 1, \dots, N\)&lt;/span&gt;.  Let &lt;span  class=&#34;math&#34;&gt;\(\mathbf x^{(i)}\)&lt;/span&gt; denote the &lt;span  class=&#34;math&#34;&gt;\(i\)&lt;/span&gt;th feature vector.  Further, let &lt;span  class=&#34;math&#34;&gt;\(\mathbf y:=(y^{(1)}, \dots, y^{(N)})\)&lt;/span&gt; denote the vector of responses.  For mathematical convenience we assume that each feature vector &lt;span  class=&#34;math&#34;&gt;\(\mathbf x^{(i)}\)&lt;/span&gt; is a &lt;span  class=&#34;math&#34;&gt;\(p+1\)&lt;/span&gt;-vector with first entry equal to &lt;span  class=&#34;math&#34;&gt;\(1\)&lt;/span&gt;.   Finally, let &lt;span  class=&#34;math&#34;&gt;\(\mathbf X\)&lt;/span&gt; denote the &lt;span  class=&#34;math&#34;&gt;\(N \times (p+1)\)&lt;/span&gt; feature matrix whose &lt;span  class=&#34;math&#34;&gt;\(i\)&lt;/span&gt;th row is just the feature vector &lt;span  class=&#34;math&#34;&gt;\(\mathbf x^{(i)}\)&lt;/span&gt;.     Given the data above, how can we find &lt;span  class=&#34;math&#34;&gt;\(\beta\)&lt;/span&gt; such that &lt;span  class=&#34;math&#34;&gt;\(\widehat Y\)&lt;/span&gt; is best fit, in some sense? One standard way to do this is to minimize the residual sum square error&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\begin{equation*}
  \text{RSS}(\beta) := \| \mathbf y - \mathbf X \beta \|^2   =(\mathbf y - \mathbf X \beta ) \cdot ( \mathbf y - \mathbf X \beta )   = \sum_{i=1}^N ( y^{(i)} - \mathbf x^{(i)} \cdot \beta)^2. 
\end{equation*}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Here &lt;span  class=&#34;math&#34;&gt;\(\mathbf X \beta\)&lt;/span&gt; denotes the usual matrix multiplication.  We can find all minimums in the usual analytical way by computing the partials of &lt;span  class=&#34;math&#34;&gt;\(\text{RSS}(\beta)\)&lt;/span&gt; with respect to the variables &lt;span  class=&#34;math&#34;&gt;\(\beta_j\)&lt;/span&gt; for &lt;span  class=&#34;math&#34;&gt;\(j=0, \dots, p\)&lt;/span&gt;.  Letting &lt;span  class=&#34;math&#34;&gt;\(\mathbf X_j\)&lt;/span&gt; denote the &lt;span  class=&#34;math&#34;&gt;\(j\)&lt;/span&gt;th column of &lt;span  class=&#34;math&#34;&gt;\(\mathbf X\)&lt;/span&gt;, we see that&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\begin{align*}
  \frac{\partial \text{RSS}}{\partial \beta_j} &amp; = 2 \sum_{i=1}^N ( y^{(i)} - \beta_j x^{(i)}_j) x^{(i)}_j \\  &amp;= 2 (\mathbf y - \beta_j \mathbf X_j) \cdot \mathbf X_j 
\end{align*}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;and so &lt;span  class=&#34;math&#34;&gt;\(\text{RSS}(\beta)\)&lt;/span&gt; is minimized when&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\begin{equation*}
  \mathbf X^T ( \mathbf y - \mathbf X \beta) = 0. 
\end{equation*}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;In the case that &lt;span  class=&#34;math&#34;&gt;\(\mathbf X^T \mathbf X\)&lt;/span&gt; is not singular the unique solution is&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\begin{equation*}
  \widehat \beta =  (\mathbf X^T \mathbf X)^{-1} \mathbf X^T \mathbf y. 
\end{equation*}\]&lt;/span&gt;&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>