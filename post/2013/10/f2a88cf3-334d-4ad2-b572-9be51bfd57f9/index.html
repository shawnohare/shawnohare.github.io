<!DOCTYPE html>
<html>
<head>
<title> Linear Models | Shawn M. O&#39;Hare</title>

<meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<meta name="description" content="">
<meta name="keywords" content="">
<meta name="author" content="Shawn O’Hare">
<meta name="generator" content="Hugo 0.55.6" />

<link rel="stylesheet" href="https://www.shawnohare.com/css/main.css">
<link rel="stylesheet" href="https://www.shawnohare.com/css/override.css">


<script type="text/javascript" src="https://www.shawnohare.com/js/mathjax.js"></script>
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full">
</script>


</head>

<body>
<nav id="site" class="ui">
  <ul>
    
    <li><a href="/">home</a></li>
    
    <li><a href="/post">posts</a></li>
    
    <li><a href="/page/about">about</a></li>
    
  </ul>
</nav>

<header>
<section class="title">
<h1>Linear Models</h1>

</section>


</header>
<main>
  <section id="content-meta">
  
<section class="content-meta ui">
<section class="date">11 October, 2013</section>



<section class="author">By <a href="https://www.shawnohare.com">Shawn O’Hare</a></section>






<section class="tags">

<a href="/tags/">Tags</a>:

<a  href="https://www.shawnohare.com/tags/statistics">statistics&nbsp;</a>

</section>




</section>


  

  </section>
  <section class="content-main"><p>Suppose we think that a random variable <span  class="math">\(Y\)</span> (the response) is linearly dependent on random variables <span  class="math">\(X_1, \dots, X_p\)</span>, where <span  class="math">\(p\)</span> is some integer.    We can model this by assuming that</p>

<p><span  class="math">\[\begin{equation*}
  Y =\underbrace{ \beta_0 + \sum_{j=1}^p \beta_j X_j}_{\widehat Y} + \epsilon,  
\end{equation*}\]</span></p>

<p>where <span  class="math">\(\beta = (\beta_0, \dots, \beta_p)\)</span> is the parameter vector for said model and <span  class="math">\(\epsilon\)</span> is a distribution for the residual error between the true value of <span  class="math">\(Y\)</span> and the linear prediction <span  class="math">\(\widehat Y\)</span>.  We assume that <span  class="math">\(\epsilon\)</span> has mean <span  class="math">\(0\)</span>. Letting <span  class="math">\(\mathbf x\)</span> denote the random vector <span  class="math">\((1,X_1, \dots, X_p)\)</span>, this model can be rewritten</p>

<p><span  class="math">\[\begin{equation*}
  Y = \mathbf x \cdot \beta + \epsilon,   \quad \text{or} \quad   y(\mathbf x ) = \mathbf x \cdot \beta + \epsilon, 
\end{equation*}\]</span></p>

<p>where <span  class="math">\(- \cdot -\)</span> represents the standard inner product on <span  class="math">\(\mathbb R^{p+1}\)</span>.    One often assumes, as we do now, that <span  class="math">\(\epsilon\)</span> has a Gaussian distribution.  The probability density function <span  class="math">\(p\)</span> for <span  class="math">\(Y\)</span> then becomes</p>

<p><span  class="math">\[\begin{equation*}
  p(y \mid \mathbf x, \beta) = \mathcal N(y \mid \mu (\mathbf x), \sigma^2 (\mathbf x)) 
\end{equation*}\]</span></p>

<p>where <span  class="math">\(\mu(\mathbf x)  = \mathbf x \cdot \beta\)</span>.  We will assume that the variance <span  class="math">\(\sigma^2(\mathbf x)\)</span> is a constant <span  class="math">\(\sigma^2\)</span>.   Suppose we make the <span  class="math">\(N\)</span> observations that <span  class="math">\(Y = y^{(i)}\)</span> when <span  class="math">\(X_j = x^{(i)}_j\)</span> for <span  class="math">\(j=1, \dots, p\)</span> and <span  class="math">\(i = 1, \dots, N\)</span>.  Let <span  class="math">\(\mathbf x^{(i)}\)</span> denote the <span  class="math">\(i\)</span>th feature vector.  Further, let <span  class="math">\(\mathbf y:=(y^{(1)}, \dots, y^{(N)})\)</span> denote the vector of responses.  For mathematical convenience we assume that each feature vector <span  class="math">\(\mathbf x^{(i)}\)</span> is a <span  class="math">\(p+1\)</span>-vector with first entry equal to <span  class="math">\(1\)</span>.   Finally, let <span  class="math">\(\mathbf X\)</span> denote the <span  class="math">\(N \times (p+1)\)</span> feature matrix whose <span  class="math">\(i\)</span>th row is just the feature vector <span  class="math">\(\mathbf x^{(i)}\)</span>.     Given the data above, how can we find <span  class="math">\(\beta\)</span> such that <span  class="math">\(\widehat Y\)</span> is best fit, in some sense? One standard way to do this is to minimize the residual sum square error</p>

<p><span  class="math">\[\begin{equation*}
  \text{RSS}(\beta) := \| \mathbf y - \mathbf X \beta \|^2   =(\mathbf y - \mathbf X \beta ) \cdot ( \mathbf y - \mathbf X \beta )   = \sum_{i=1}^N ( y^{(i)} - \mathbf x^{(i)} \cdot \beta)^2. 
\end{equation*}\]</span></p>

<p>Here <span  class="math">\(\mathbf X \beta\)</span> denotes the usual matrix multiplication.  We can find all minimums in the usual analytical way by computing the partials of <span  class="math">\(\text{RSS}(\beta)\)</span> with respect to the variables <span  class="math">\(\beta_j\)</span> for <span  class="math">\(j=0, \dots, p\)</span>.  Letting <span  class="math">\(\mathbf X_j\)</span> denote the <span  class="math">\(j\)</span>th column of <span  class="math">\(\mathbf X\)</span>, we see that</p>

<p><span  class="math">\[\begin{align*}
  \frac{\partial \text{RSS}}{\partial \beta_j} & = 2 \sum_{i=1}^N ( y^{(i)} - \beta_j x^{(i)}_j) x^{(i)}_j \\  &= 2 (\mathbf y - \beta_j \mathbf X_j) \cdot \mathbf X_j 
\end{align*}\]</span></p>

<p>and so <span  class="math">\(\text{RSS}(\beta)\)</span> is minimized when</p>

<p><span  class="math">\[\begin{equation*}
  \mathbf X^T ( \mathbf y - \mathbf X \beta) = 0. 
\end{equation*}\]</span></p>

<p>In the case that <span  class="math">\(\mathbf X^T \mathbf X\)</span> is not singular the unique solution is</p>

<p><span  class="math">\[\begin{equation*}
  \widehat \beta =  (\mathbf X^T \mathbf X)^{-1} \mathbf X^T \mathbf y. 
\end{equation*}\]</span></p>

<p>Assuming that the observed data is independently and identically distributed,  the log-likelihood function for the given data <span  class="math">\( \mathcal D := \{ \mathbf y, \mathbf X \}\)</span> is</p>

<p><span  class="math">\[\begin{align*}
\label{eq:likelihood}  \ell (\beta) &:= \ln p (\mathcal D \mid \beta) \\    &= \sum_{i=1}^N \ln p(y^{(i)} \mid x^{(i)} , \beta) \\     &= \sum_{i=1}^{N} \ln \left[ \frac{1}{\sqrt{2 \pi \sigma^2}} \exp \left( - \frac{1}{2 \sigma^2} (y^{(i)}  - \mathbf x^{(i)} \cdot \beta)^2 \right ) \right] . 
\end{align*}\]</span></p>

<p>It is easy to see that <span  class="math">\(\widehat \beta\)</span> is a maximum likelihood estimate for the data, i.e., <span  class="math">\(\widehat \beta\)</span> satisfies</p>

<p><span  class="math">\[\begin{equation*}
  \widehat \beta = \arg \max_{\beta} \ell (\beta).  
\end{equation*}\]</span></p>

<p>We can in turn of course suppose that the data <span  class="math">\(\mathcal D\)</span> varies and consider the maximum likelihood estimator <span  class="math">\(\widehat \beta( \mathcal D)\)</span>, which is a random variable itself. The variance-covariance matrix for the maximum likelihood estimator <span  class="math">\(\widehat \beta\)</span> is:</p>

<p><span  class="math">\[\begin{equation*}
  \text{Var}(\widehat \beta) = (\mathbf X^T \mathbf X)^{-1} \sigma^2. 
\end{equation*}\]</span></p>

<p>We can estimate <span  class="math">\(\sigma^2\)</span> by</p>

<p><span  class="math">\[\begin{equation*}
  \widehat \sigma^2 = \frac{1}{N-p-1} \sum_{i=1}^N (y^{(i)} - \widehat y^{(i)})^2, 
\end{equation*}\]</span></p>

<p>where <span  class="math">\( \widehat y^{(i)} := \mathbf x^{(i)} \cdot \widehat \beta\)</span> is the response prediction of the best fit linear model. This is an unbiased estimator for <span  class="math">\(\sigma^2\)</span>, which means that the expectation <span  class="math">\(E (\widehat \sigma^2) = \sigma^2\)</span>.</p></section>
</main>

<footer class="ui" id="site">
  <ul>
    
  </ul>

  
  <section id="copyright">© Shawn M. O’Hare.  <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Some rights reserved</a>.</section>
</footer>
</body>
</html>
